{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AniketGaudgaul/AI-ML-DL_Projects/blob/main/T5_Fine_Tuning_Text_Summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zosJFp1B-TT",
        "outputId": "fce1e5fa-23b0-42b9-9218-caa80ab5f5d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ],
      "source": [
        "! pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -q\n",
        "!pip install wandb -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDE4WXOD_36Q",
        "outputId": "cf4f4110-f8fd-4f10-bc63-d6a8a44116e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.3/203.3 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import io\n",
        "\n",
        "\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration"
      ],
      "metadata": {
        "id": "Gs1Kq6TOALfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, source_len, summ_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.source_len = source_len\n",
        "        self.summ_len = summ_len\n",
        "        self.text = self.data.text\n",
        "        self.ctext = self.data.ctext\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        ctext = str(self.ctext[index])\n",
        "        ctext = ' '.join(ctext.split())\n",
        "\n",
        "        text = str(self.text[index])\n",
        "        text = ' '.join(text.split())\n",
        "\n",
        "        source = self.tokenizer.batch_encode_plus([ctext], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt')\n",
        "        target = self.tokenizer.batch_encode_plus([text], max_length= self.summ_len, pad_to_max_length=True,return_tensors='pt')\n",
        "\n",
        "        source_ids = source['input_ids'].squeeze()\n",
        "        source_mask = source['attention_mask'].squeeze()\n",
        "        target_ids = target['input_ids'].squeeze()\n",
        "        target_mask = target['attention_mask'].squeeze()\n",
        "\n",
        "        return {\n",
        "            'source_ids': source_ids.to(dtype=torch.long),\n",
        "            'source_mask': source_mask.to(dtype=torch.long),\n",
        "            'target_ids': target_ids.to(dtype=torch.long),\n",
        "            'target_ids_y': target_ids.to(dtype=torch.long)\n",
        "        }"
      ],
      "metadata": {
        "id": "qIrihd2IASf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
        "    model.train()\n",
        "    for _,data in enumerate(loader, 0):\n",
        "        y = data['target_ids'].to(device, dtype=torch.long)\n",
        "        y_ids = y[:, :-1].contiguous()\n",
        "        labels = y[:, 1:].clone().detach()\n",
        "        labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "        ids = data['source_ids'].to(device, dtype=torch.long)\n",
        "        mask = data['source_mask'].to(device, dtype=torch.long)\n",
        "\n",
        "        outputs = model(input_ids=ids, attention_mask=mask, decoder_input_ids=y_ids, labels=labels)\n",
        "        loss = outputs[0]\n",
        "\n",
        "        if _ % 10 == 0:\n",
        "            print(f'Training Loss: {loss.item()}' , ' Epoch: ' , _)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n"
      ],
      "metadata": {
        "id": "cEkNnLSBAa-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(epoch, tokenizer, model, device, loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(loader, 0):\n",
        "            y = data['target_ids'].to(device, dtype = torch.long)\n",
        "            ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "            generated_ids = model.generate(\n",
        "                input_ids = ids,\n",
        "                attention_mask = mask,\n",
        "                max_length=150,\n",
        "                num_beams=2,\n",
        "                repetition_penalty=2.5,\n",
        "                length_penalty=1.0,\n",
        "                early_stopping=True\n",
        "                )\n",
        "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
        "            if _%100==0:\n",
        "                print(f'Completed {_}')\n",
        "\n",
        "            predictions.extend(preds)\n",
        "            actuals.extend(target)\n",
        "    return predictions, actuals"
      ],
      "metadata": {
        "id": "jyANl5qpAbZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install easydict\n",
        "from easydict import EasyDict\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfs_Lp_dAn2K",
        "outputId": "6e093b39-d68c-4206-82c7-0faf19dfc49d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: easydict in /usr/local/lib/python3.10/dist-packages (1.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5oPcuYLBLjj",
        "outputId": "f32def81-1299-414c-ce6f-9ebadacabe8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.13)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2022.12.7)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.65.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.26.15)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle datasets download sunnysai12345/news-summary\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjCYuwLZBTgD",
        "outputId": "4b309776-481e-4366-e6d9-ae4b9e1c43db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading news-summary.zip to /content\n",
            " 46% 9.00M/19.8M [00:00<00:00, 37.6MB/s]\n",
            "100% 19.8M/19.8M [00:00<00:00, 70.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip news-summary.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lF--UUwvBd8Z",
        "outputId": "9ed8db4f-7a4b-4e5a-fe26-6b7f3849f6c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  news-summary.zip\n",
            "  inflating: news_summary.csv        \n",
            "  inflating: news_summary_more.csv   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "fXXNf6G9ClMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "config = EasyDict()\n",
        "config.TRAIN_BATCH_SIZE = 2   # input batch size for training (default: 64)\n",
        "config.VALID_BATCH_SIZE = 2  # input batch size for testing (default: 1000)\n",
        "config.TRAIN_EPOCHS = 2       # number of epochs to train (default: 10)\n",
        "config.VAL_EPOCHS = 1\n",
        "config.LEARNING_RATE = 1e-4    # learning rate (default: 0.01)\n",
        "config.SEED = 42               # random seed (default: 42)\n",
        "config.MAX_LEN = 512\n",
        "config.SUMMARY_LEN = 150\n",
        "\n",
        "# Set random seeds and deterministic pytorch for reproducibility\n",
        "torch.manual_seed(config.SEED) # pytorch random seed\n",
        "np.random.seed(config.SEED) # numpy random seed\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# tokenzier for encoding the text\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "\n",
        "df = pd.read_csv(('news_summary.csv'),encoding='latin-1')\n",
        "df = df[['text','ctext']]\n",
        "df.ctext = 'summarize: ' + df.ctext\n",
        "print(df.head())\n",
        "train_size = 0.8\n",
        "train_dataset=df.sample(frac=train_size,random_state = config.SEED)\n",
        "val_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
        "train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(df.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "print(\"TEST Dataset: {}\".format(val_dataset.shape))\n",
        "\n",
        "training_set = CustomDataset(train_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n",
        "val_set = CustomDataset(val_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n",
        "\n",
        "train_params = {\n",
        "    'batch_size': config.TRAIN_BATCH_SIZE,\n",
        "    'shuffle': True,\n",
        "    'num_workers': 0\n",
        "    }\n",
        "\n",
        "val_params = {\n",
        "    'batch_size': config.VALID_BATCH_SIZE,\n",
        "    'shuffle': False,\n",
        "    'num_workers': 0\n",
        "    }\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "val_loader = DataLoader(val_set, **val_params)\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=config.LEARNING_RATE)\n",
        "\n",
        "print('Initiating Fine-Tuning for the model on our dataset')\n",
        "\n",
        "for epoch in range(config.TRAIN_EPOCHS):\n",
        "    train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
        "\n",
        "print('Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe')\n",
        "for epoch in range(config.VAL_EPOCHS):\n",
        "    predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
        "    final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
        "    print('Output Files generated for review')\n",
        "\n",
        "print(final_df)\n",
        "final_df.to_csv(\"final_df.csv\", index=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7ev01LiAdys",
        "outputId": "d1077172-b879-4fe0-d072-cf1ec65a7eef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  \\\n",
            "0  The Administration of Union Territory Daman an...   \n",
            "1  Malaika Arora slammed an Instagram user who tr...   \n",
            "2  The Indira Gandhi Institute of Medical Science...   \n",
            "3  Lashkar-e-Taiba's Kashmir commander Abu Dujana...   \n",
            "4  Hotels in Maharashtra will train their staff t...   \n",
            "\n",
            "                                               ctext  \n",
            "0  summarize: The Daman and Diu administration on...  \n",
            "1  summarize: From her special numbers to TV?appe...  \n",
            "2  summarize: The Indira Gandhi Institute of Medi...  \n",
            "3  summarize: Lashkar-e-Taiba's Kashmir commander...  \n",
            "4  summarize: Hotels in Mumbai and other Indian c...  \n",
            "FULL Dataset: (4514, 2)\n",
            "TRAIN Dataset: (3611, 2)\n",
            "TEST Dataset: (903, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initiating Fine-Tuning for the model on our dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 6.244161128997803\n",
            "Training Loss: 3.624645471572876\n",
            "Training Loss: 2.6974895000457764\n",
            "Training Loss: 2.487234592437744\n",
            "Training Loss: 1.7650047540664673\n",
            "Training Loss: 2.5718820095062256\n",
            "Training Loss: 2.045772075653076\n",
            "Training Loss: 2.4007930755615234\n",
            "Training Loss: 2.069049835205078\n",
            "Training Loss: 2.2349841594696045\n",
            "Training Loss: 1.815329670906067\n",
            "Training Loss: 1.9482977390289307\n",
            "Training Loss: 1.1682369709014893\n",
            "Training Loss: 2.752784490585327\n",
            "Training Loss: 3.1725101470947266\n",
            "Training Loss: 2.755039930343628\n",
            "Training Loss: 2.0997426509857178\n",
            "Training Loss: 2.6514477729797363\n",
            "Training Loss: 1.9437199831008911\n",
            "Training Loss: 1.7017844915390015\n",
            "Training Loss: 2.0583715438842773\n",
            "Training Loss: 2.2996437549591064\n",
            "Training Loss: 2.611858606338501\n",
            "Training Loss: 2.1015541553497314\n",
            "Training Loss: 2.2763020992279053\n",
            "Training Loss: 1.548233985900879\n",
            "Training Loss: 1.8016631603240967\n",
            "Training Loss: 2.2198386192321777\n",
            "Training Loss: 1.4324060678482056\n",
            "Training Loss: 1.7342751026153564\n",
            "Training Loss: 1.4921702146530151\n",
            "Training Loss: 1.476580262184143\n",
            "Training Loss: 1.781891942024231\n",
            "Training Loss: 1.1933796405792236\n",
            "Training Loss: 2.234410285949707\n",
            "Training Loss: 1.7271572351455688\n",
            "Training Loss: 2.0853939056396484\n",
            "Training Loss: 1.922343373298645\n",
            "Training Loss: 1.806369662284851\n",
            "Training Loss: 1.5158392190933228\n",
            "Training Loss: 2.106741189956665\n",
            "Training Loss: 2.6469168663024902\n",
            "Training Loss: 2.1113269329071045\n",
            "Training Loss: 1.9014610052108765\n",
            "Training Loss: 1.93955397605896\n",
            "Training Loss: 2.0818982124328613\n",
            "Training Loss: 1.7996242046356201\n",
            "Training Loss: 1.6845409870147705\n",
            "Training Loss: 1.8336071968078613\n",
            "Training Loss: 1.5966960191726685\n",
            "Training Loss: 1.7460999488830566\n",
            "Training Loss: 1.5092594623565674\n",
            "Training Loss: 1.6789051294326782\n",
            "Training Loss: 1.5932207107543945\n",
            "Training Loss: 1.4884037971496582\n",
            "Training Loss: 1.564933180809021\n",
            "Training Loss: 0.9568020701408386\n",
            "Training Loss: 1.7384976148605347\n",
            "Training Loss: 1.9599138498306274\n",
            "Training Loss: 1.7194594144821167\n",
            "Training Loss: 1.8929896354675293\n",
            "Training Loss: 2.3568413257598877\n",
            "Training Loss: 1.956535816192627\n",
            "Training Loss: 1.5844424962997437\n",
            "Training Loss: 1.6819573640823364\n",
            "Training Loss: 1.5704013109207153\n",
            "Training Loss: 1.268835186958313\n",
            "Training Loss: 1.7727676630020142\n",
            "Training Loss: 1.9971362352371216\n",
            "Training Loss: 1.8865336179733276\n",
            "Training Loss: 1.389573097229004\n",
            "Training Loss: 2.257338523864746\n",
            "Training Loss: 1.7100391387939453\n",
            "Training Loss: 1.5129808187484741\n",
            "Training Loss: 1.7515519857406616\n",
            "Training Loss: 1.1398857831954956\n",
            "Training Loss: 1.2841174602508545\n",
            "Training Loss: 1.7913618087768555\n",
            "Training Loss: 2.6121041774749756\n",
            "Training Loss: 2.055788516998291\n",
            "Training Loss: 2.501858949661255\n",
            "Training Loss: 1.4977571964263916\n",
            "Training Loss: 1.8189966678619385\n",
            "Training Loss: 1.9989168643951416\n",
            "Training Loss: 1.497410535812378\n",
            "Training Loss: 1.3514941930770874\n",
            "Training Loss: 2.242331027984619\n",
            "Training Loss: 1.359479308128357\n",
            "Training Loss: 1.5375986099243164\n",
            "Training Loss: 1.6228238344192505\n",
            "Training Loss: 1.9164133071899414\n",
            "Training Loss: 1.8478010892868042\n",
            "Training Loss: 1.651857614517212\n",
            "Training Loss: 2.0038902759552\n",
            "Training Loss: 1.9300107955932617\n",
            "Training Loss: 1.0926897525787354\n",
            "Training Loss: 2.1796205043792725\n",
            "Training Loss: 1.7186849117279053\n",
            "Training Loss: 1.3862614631652832\n",
            "Training Loss: 1.4020969867706299\n",
            "Training Loss: 1.161633849143982\n",
            "Training Loss: 2.438917875289917\n",
            "Training Loss: 1.5245105028152466\n",
            "Training Loss: 1.5863548517227173\n",
            "Training Loss: 1.4540936946868896\n",
            "Training Loss: 1.5620237588882446\n",
            "Training Loss: 1.7291181087493896\n",
            "Training Loss: 1.4396560192108154\n",
            "Training Loss: 1.32626473903656\n",
            "Training Loss: 1.2583229541778564\n",
            "Training Loss: 1.8974671363830566\n",
            "Training Loss: 1.4609142541885376\n",
            "Training Loss: 1.8921761512756348\n",
            "Training Loss: 1.7902830839157104\n",
            "Training Loss: 2.028709888458252\n",
            "Training Loss: 1.7444233894348145\n",
            "Training Loss: 2.4086506366729736\n",
            "Training Loss: 1.8750876188278198\n",
            "Training Loss: 1.158629059791565\n",
            "Training Loss: 1.5848218202590942\n",
            "Training Loss: 1.533255934715271\n",
            "Training Loss: 1.9421294927597046\n",
            "Training Loss: 2.059465169906616\n",
            "Training Loss: 1.8037253618240356\n",
            "Training Loss: 1.160436749458313\n",
            "Training Loss: 1.7543810606002808\n",
            "Training Loss: 2.4135680198669434\n",
            "Training Loss: 0.9091952443122864\n",
            "Training Loss: 1.8518441915512085\n",
            "Training Loss: 1.6622613668441772\n",
            "Training Loss: 1.5575644969940186\n",
            "Training Loss: 1.961380958557129\n",
            "Training Loss: 1.423596739768982\n",
            "Training Loss: 1.6612489223480225\n",
            "Training Loss: 1.721798300743103\n",
            "Training Loss: 1.13921058177948\n",
            "Training Loss: 1.2569459676742554\n",
            "Training Loss: 1.6754354238510132\n",
            "Training Loss: 1.7998448610305786\n",
            "Training Loss: 1.3639638423919678\n",
            "Training Loss: 1.1488772630691528\n",
            "Training Loss: 1.534507393836975\n",
            "Training Loss: 1.6545454263687134\n",
            "Training Loss: 1.575028419494629\n",
            "Training Loss: 1.134622573852539\n",
            "Training Loss: 1.5609540939331055\n",
            "Training Loss: 1.0694208145141602\n",
            "Training Loss: 0.9427488446235657\n",
            "Training Loss: 2.072676420211792\n",
            "Training Loss: 1.2206416130065918\n",
            "Training Loss: 2.035656213760376\n",
            "Training Loss: 1.274954080581665\n",
            "Training Loss: 1.2724238634109497\n",
            "Training Loss: 1.5762240886688232\n",
            "Training Loss: 1.3239555358886719\n",
            "Training Loss: 1.6564255952835083\n",
            "Training Loss: 1.492676854133606\n",
            "Training Loss: 1.2353800535202026\n",
            "Training Loss: 1.5713269710540771\n",
            "Training Loss: 1.8981351852416992\n",
            "Training Loss: 1.658996820449829\n",
            "Training Loss: 1.2972743511199951\n",
            "Training Loss: 1.035928726196289\n",
            "Training Loss: 1.7176848649978638\n",
            "Training Loss: 2.2669312953948975\n",
            "Training Loss: 1.4409981966018677\n",
            "Training Loss: 2.1655352115631104\n",
            "Training Loss: 1.516963005065918\n",
            "Training Loss: 2.049938440322876\n",
            "Training Loss: 2.067502975463867\n",
            "Training Loss: 1.4739112854003906\n",
            "Training Loss: 1.3401858806610107\n",
            "Training Loss: 0.9380909204483032\n",
            "Training Loss: 1.3157035112380981\n",
            "Training Loss: 1.8426991701126099\n",
            "Training Loss: 1.5029401779174805\n",
            "Training Loss: 1.6076738834381104\n",
            "Training Loss: 1.3052579164505005\n",
            "Training Loss: 1.3339968919754028\n",
            "Training Loss: 2.0583043098449707\n",
            "Training Loss: 1.5803639888763428\n",
            "Training Loss: 1.504172444343567\n",
            "Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe\n",
            "Completed 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed 100\n",
            "Completed 200\n",
            "Output Files generated for review\n",
            "                                        Generated Text  \\\n",
            "0    Mumbai and other Indian cities are to train th...   \n",
            "1    UP Congress Party has opened a 'State Bank of ...   \n",
            "2    a 24-year-old Indian athlete has been indicted...   \n",
            "3    the remains of a German hiker who disappeared ...   \n",
            "4    GP Manish Shah, who practised in east London, ...   \n",
            "..                                                 ...   \n",
            "898  HSBC has disclosed being probed by tax authori...   \n",
            "899  NHAI has chopped more than 8,000 trees to deve...   \n",
            "900  Earlier this month, the Indian government anno...   \n",
            "901  'Rangoon' stars Kangana Ranaut, Shahid Kapoor ...   \n",
            "902  Earlier this month, the Indian government anno...   \n",
            "\n",
            "                                           Actual Text  \n",
            "0    Hotels in Maharashtra will train their staff t...  \n",
            "1    The Congress party has opened a bank called 'S...  \n",
            "2    Tanveer Hussain, a 24-year-old Indian athlete ...  \n",
            "3    The remains of a German hiker, who disappeared...  \n",
            "4    A UK-based doctor, Manish Shah, has been charg...  \n",
            "..                                                 ...  \n",
            "898  Global banking giant HSBC has disclosed being ...  \n",
            "899  The National Highways Authority of India (NHAI...  \n",
            "900  Former Infosys CFO Mohandas Pai has said that ...  \n",
            "901  The Kangana Ranaut, Shahid Kapoor and Saif Ali...  \n",
            "902  A ticket collector on Thursday allegedly bit o...  \n",
            "\n",
            "[903 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(final_df.iloc[1,:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nw8FKUS0CaKF",
        "outputId": "3c664b3a-0410-442d-af08-62ad487b0ac6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text    UP Congress Party has opened a 'State Bank of ...\n",
            "Actual Text       The Congress party has opened a bank called 'S...\n",
            "Name: 1, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XchHIWsHPB5n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}